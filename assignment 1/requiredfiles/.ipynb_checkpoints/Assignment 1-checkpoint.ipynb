{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "corr = None\n",
    "ifTest = False\n",
    "Ztr = {}\n",
    "Zte = {}\n",
    "\n",
    "\n",
    "def returnNext(state, action):\n",
    "    # Returns (priceChange, nextState) as a tuple\n",
    "    #nextState is actually a 2-tuple representing the state\n",
    "    a,b = state\n",
    "    L = []\n",
    "    if ifTest:\n",
    "        L = Zte[(a,b)]\n",
    "    else:\n",
    "        L = Ztr[(a,b)]\n",
    "    r = L[random.randint(0,len(L)-1)]\n",
    "    pricc = None\n",
    "    if action==-1:\n",
    "        if r[0]==0:\n",
    "            pricc =  r[4]\n",
    "        elif r[0]==1:\n",
    "            pricc =  r[3]\n",
    "        else:\n",
    "            pricc =  -r[3]\n",
    "    elif action==1:\n",
    "        if r[2]==0:\n",
    "            pricc =  r[4]\n",
    "        elif r[2]==1:\n",
    "            pricc =  r[3]\n",
    "        else:\n",
    "            pricc =  -r[3]\n",
    "    else:\n",
    "        if r[1]==0:\n",
    "            pricc =  r[4]\n",
    "        elif r[1]==1:\n",
    "            pricc =  r[3]\n",
    "        else:\n",
    "            pricc =  -r[3]\n",
    "    num = 3*(10*a + b) + action + 1\n",
    "\n",
    "    F = corr[num,:]\n",
    "    ch = np.random.choice(np.arange(0,100),p = list(F))\n",
    "    ca = ch // 10\n",
    "    cb = ch % 10\n",
    "    return (pricc,(ca,cb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ftr = input('Test : Filename to read from?')\n",
    "# fte = input('Train : Filename to read?')\n",
    "\n",
    "fte =\"./test.pic\"\n",
    "ftr =\"./train.pic\"\n",
    "\n",
    "\n",
    "with open(ftr,'rb') as f:\n",
    "    Ztr = pickle.load(f)\n",
    "\n",
    "with open(fte,'rb') as f:\n",
    "    Zte = pickle.load(f)\n",
    "\n",
    "with open('correlations.npy','rb') as f:\n",
    "    corr = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of States and Actions\n",
    "n_states =100\n",
    "n_actions = 3\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "Q-Learning is an Off-Policy algorithm for Temporal Difference learning. It can be proven that given sufficient training under any $\\epsilon$-soft policy, the algorithm converges with probability 1 to a close approximation of the action-value function for an arbitrary target policy. Q-Learning learns the optimal policy even when actions are selected according to a more exploratory or even random policy.\n",
    "\n",
    "So, We are going to use Q-Learning to train our Markov Chain to take best possible action $\\textbf{a}$ in the given state $\\textbf{s}$. We start off by first declaring a matrix Q, of dimension $n_{states} \\times n_{actions}$, which we are going to train to take decisions for us in a given state $\\textbf{s}$\n",
    "\n",
    "$$lr~ (\\alpha)=0.1$$\n",
    "$$gamma ~(\\gamma) =0.99$$\n",
    "$$ epsilon ~(\\epsilon) =0.9 $$\n",
    "$$epsilon\\_decay =0.01$$\n",
    "$$epsilon\\_final = 0.001$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial Q-table\n",
    "Q = np.zeros([n_states, n_actions])\n",
    "epsilons = []\n",
    "cum_rewards=[]\n",
    "\n",
    "# learning rate\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount factor\n",
    "gamma = 0.99\n",
    "# e-greedy exploitation\n",
    "epsilon = 0.9\n",
    "epsilon_decay = 0.01\n",
    "epsilon_final = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "n_episodes = 2000\n",
    "n_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getState(state, epsilon):\n",
    "    \"\"\"this function returns state based on the value of random number generated\"\"\"\n",
    "    global Q\n",
    "    p = np.random.uniform(0,1)\n",
    "    \n",
    "    action =None\n",
    "    if p>epsilon:\n",
    "        rand_values = Q[state]\n",
    "        action = np.argmax(rand_values)-1\n",
    "    else:\n",
    "        action = np.random.randint(n_actions)-1\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinIndex_Value(l):\n",
    "    min_value = min(l)\n",
    "    min_index = l.index(min_value)\n",
    "    \n",
    "    return min_value, min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:13<00:00, 147.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train your Markov Decision Process, make use of the returnNext function to model and act on the priceChanges dependence on the states and actions\n",
    "def Train():\n",
    "    \n",
    "    global n_episodes, n_steps, epsilon, epsilon_decay, epsilon_final, gamma, alpha, epsilons, Q\n",
    "    \n",
    "    for i in tqdm(range(n_episodes)):\n",
    "        # randomly generaating states between 0 and 100\n",
    "        state = random.randint(0,99)\n",
    "        # initial cumulative reward set\n",
    "        cum_reward =0\n",
    "        # initial price of bond\n",
    "        inventory=[]\n",
    "        price =100\n",
    "        balance = 0\n",
    "        bondbal = 0\n",
    "        networth = 0\n",
    "        \n",
    "        for j in range(n_steps):\n",
    "            # performing n_steps within each iteration\n",
    "            p = np.random.rand()\n",
    "            \n",
    "            action = None\n",
    "            if p>epsilon:\n",
    "                rand_values = Q[state]\n",
    "                action = np.argmax(rand_values)-1\n",
    "            else:\n",
    "                action = np.random.randint(n_actions)-1\n",
    "            \n",
    "            picc, nxt_state = returnNext((state//10,state%10), action)\n",
    "            \n",
    "            new_state =nxt_state[0]*10+nxt_state[1]\n",
    "            \n",
    "            old_networth = networth\n",
    "            \n",
    "            reward =0\n",
    "            price+=picc\n",
    "            if action==1:\n",
    "                inventory.append(price)\n",
    "                balance-=10*price\n",
    "                bondbal+=10\n",
    "            elif action ==-1 and len(inventory)>0:\n",
    "                balance+=10*price\n",
    "                bondbal-=10\n",
    "            \n",
    "            networth = balance + bondbal*price\n",
    "            delta = networth - old_networth\n",
    "            reward =delta\n",
    "#             reward = max(delta,0)\n",
    "            \n",
    "            Q[state][action+1] = ((1-alpha)*Q[state][action+1]) + alpha*(reward + gamma*np.max(Q[new_state]))\n",
    "            \n",
    "            cum_reward+=reward\n",
    "            state = new_state\n",
    "        \n",
    "        \n",
    "        if epsilon > epsilon_final:\n",
    "                epsilon*=(1-epsilon_decay)\n",
    "                epsilons.append(epsilon)\n",
    "        cum_rewards.append(cum_reward)\n",
    "        \n",
    "ifTest  = False\n",
    "Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 : mean espiode reward:  97.17470198639934\n",
      "200 : mean espiode reward:  815.2496620388232\n",
      "300 : mean espiode reward:  753.8025774131397\n",
      "400 : mean espiode reward:  694.6271299080213\n",
      "500 : mean espiode reward:  1087.6620075890353\n",
      "600 : mean espiode reward:  1054.6013208550444\n",
      "700 : mean espiode reward:  968.1742399904266\n",
      "800 : mean espiode reward:  991.8479593327571\n",
      "900 : mean espiode reward:  944.3620987445007\n",
      "1000 : mean espiode reward:  882.4573485274982\n",
      "1100 : mean espiode reward:  984.9932932868599\n",
      "1200 : mean espiode reward:  911.8143534715718\n",
      "1300 : mean espiode reward:  958.8562667462883\n",
      "1400 : mean espiode reward:  1025.3360312764216\n",
      "1500 : mean espiode reward:  976.8046605383477\n",
      "1600 : mean espiode reward:  941.6191266940415\n",
      "1700 : mean espiode reward:  927.3681896534796\n",
      "1800 : mean espiode reward:  999.0413030269707\n",
      "1900 : mean espiode reward:  1080.0506252533223\n",
      "2000 : mean espiode reward:  991.5221802846122\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print((i+1)*100,\": mean espiode reward: \",\\\n",
    "           np.mean(cum_rewards[100*i:100*(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# This you need to write after training your MDP, this should just take in the state and return the Action you would perform\n",
    "# Please do not make use of the returnNext function inside here, that would defeat the purpose of training the model, as it would be known to you!\n",
    "\n",
    "def Run(state):\n",
    "    global epsilon\n",
    "    s = state[0]*10+ state[1]%10\n",
    "    action =getState(s, epsilon)\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main function, you don't need to tamper with it!\n",
    "def mainRun(iter = 1000):\n",
    "    initstate = (random.randint(0,9),random.randint(0,9))\n",
    "    i = 0\n",
    "    initprice = 100.00\n",
    "    price = initprice\n",
    "    balance = 0\n",
    "    bondbal = 0\n",
    "    networth = 0\n",
    "    st = initstate\n",
    "    while i < iter:\n",
    "        act = Run(st)\n",
    "        pricCh, ns = returnNext(st,act)\n",
    "        price += pricCh\n",
    "        if act==1:\n",
    "            balance -= 10*price\n",
    "            bondbal += 10\n",
    "        elif act==-1:\n",
    "            balance += 10*price\n",
    "            bondbal -= 10\n",
    "        \n",
    "        networth = balance + bondbal*price\n",
    "        st = ns\n",
    "        if i%(iter//10) ==0:\n",
    "            print('Your Networth has went from 0 to ',networth)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Networth has went from 0 to  0.0\n",
      "Your Networth has went from 0 to  -16.641611909344647\n",
      "Your Networth has went from 0 to  600.5418212118711\n",
      "Your Networth has went from 0 to  2986.6999072877043\n",
      "Your Networth has went from 0 to  3985.29936897476\n",
      "Your Networth has went from 0 to  6658.352418057904\n",
      "Your Networth has went from 0 to  10831.746360472389\n",
      "Your Networth has went from 0 to  15414.767571776538\n",
      "Your Networth has went from 0 to  20267.80550587458\n",
      "Your Networth has went from 0 to  19545.625301151114\n"
     ]
    }
   ],
   "source": [
    "ifTest =True\n",
    "mainRun(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the output above, for **iter=100000**, the networth keeps on increasing in long term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Q3.npy',Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.load('Q.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
