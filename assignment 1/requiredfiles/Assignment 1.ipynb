{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "corr = None\n",
    "ifTest = False\n",
    "Ztr = {}\n",
    "Zte = {}\n",
    "\n",
    "\n",
    "def returnNext(state, action):\n",
    "    # Returns (priceChange, nextState) as a tuple\n",
    "    #nextState is actually a 2-tuple representing the state\n",
    "    a,b = state\n",
    "    L = []\n",
    "    if ifTest:\n",
    "        L = Zte[(a,b)]\n",
    "    else:\n",
    "        L = Ztr[(a,b)]\n",
    "    r = L[random.randint(0,len(L)-1)]\n",
    "    pricc = None\n",
    "    if action==-1:\n",
    "        if r[0]==0:\n",
    "            pricc =  r[4]\n",
    "        elif r[0]==1:\n",
    "            pricc =  r[3]\n",
    "        else:\n",
    "            pricc =  -r[3]\n",
    "    elif action==1:\n",
    "        if r[2]==0:\n",
    "            pricc =  r[4]\n",
    "        elif r[2]==1:\n",
    "            pricc =  r[3]\n",
    "        else:\n",
    "            pricc =  -r[3]\n",
    "    else:\n",
    "        if r[1]==0:\n",
    "            pricc =  r[4]\n",
    "        elif r[1]==1:\n",
    "            pricc =  r[3]\n",
    "        else:\n",
    "            pricc =  -r[3]\n",
    "    num = 3*(10*a + b) + action + 1\n",
    "\n",
    "    F = corr[num,:]\n",
    "    ch = np.random.choice(np.arange(0,100),p = list(F))\n",
    "    ca = ch // 10\n",
    "    cb = ch % 10\n",
    "    return (pricc,(ca,cb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ftr = input('Test : Filename to read from?')\n",
    "# fte = input('Train : Filename to read?')\n",
    "\n",
    "fte =\"./test.pic\"\n",
    "ftr =\"./train.pic\"\n",
    "\n",
    "\n",
    "with open(ftr,'rb') as f:\n",
    "    Ztr = pickle.load(f)\n",
    "\n",
    "with open(fte,'rb') as f:\n",
    "    Zte = pickle.load(f)\n",
    "\n",
    "with open('correlations.npy','rb') as f:\n",
    "    corr = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of States and Actions\n",
    "n_states =100\n",
    "n_actions = 3\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "Q-Learning is an Off-Policy algorithm for Temporal Difference learning. It can be proven that given sufficient training under any $\\epsilon$-soft policy, the algorithm converges with probability 1 to a close approximation of the action-value function for an arbitrary target policy. Q-Learning learns the optimal policy even when actions are selected according to a more exploratory or even random policy.\n",
    "\n",
    "So, We are going to use Q-Learning to train our Markov Chain to take best possible action $\\textbf{a}$ in the given state $\\textbf{s}$. We start off by first declaring a matrix Q, of dimension $n_{states} \\times n_{actions}$, which we are going to train to take decisions for us in a given state $\\textbf{s}$\n",
    "\n",
    "$$lr~ (\\alpha)=0.1$$\n",
    "$$gamma ~(\\gamma) =0.99$$\n",
    "$$ epsilon ~(\\epsilon) =0.9 $$\n",
    "$$epsilon\\_decay =0.01$$\n",
    "$$epsilon\\_final = 0.001$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial Q-table\n",
    "Q = np.zeros([n_states, n_actions])\n",
    "epsilons = []\n",
    "cum_rewards=[]\n",
    "\n",
    "# learning rate\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount factor\n",
    "gamma = 0.99\n",
    "# e-greedy exploitation\n",
    "epsilon = 0.9\n",
    "epsilon_decay = 0.01\n",
    "epsilon_final = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "n_episodes = 2000\n",
    "n_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getState(state, epsilon):\n",
    "    \"\"\"this function returns state based on the value of random number generated\"\"\"\n",
    "    global Q\n",
    "    p = np.random.uniform(0,1)\n",
    "    \n",
    "    action =None\n",
    "    if p>epsilon:\n",
    "        rand_values = Q[state]\n",
    "        action = np.argmax(rand_values)-1\n",
    "    else:\n",
    "        action = np.random.randint(n_actions)-1\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinIndex_Value(l):\n",
    "    min_value = min(l)\n",
    "    min_index = l.index(min_value)\n",
    "    \n",
    "    return min_value, min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:13<00:00, 145.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train your Markov Decision Process, make use of the returnNext function to model and act on the priceChanges dependence on the states and actions\n",
    "def Train():\n",
    "    \n",
    "    global n_episodes, n_steps, epsilon, epsilon_decay, epsilon_final, gamma, alpha, epsilons, Q\n",
    "    \n",
    "    for i in tqdm(range(n_episodes)):\n",
    "        # randomly generaating states between 0 and 100\n",
    "        state = random.randint(0,99)\n",
    "        # initial cumulative reward set\n",
    "        cum_reward =0\n",
    "        # initial price of bond\n",
    "        inventory=[]\n",
    "        price =100\n",
    "        balance = 0\n",
    "        bondbal = 0\n",
    "        networth = 0\n",
    "        \n",
    "        for j in range(n_steps):\n",
    "            # performing n_steps within each iteration\n",
    "            p = np.random.rand()\n",
    "            \n",
    "            action = None\n",
    "            if p>epsilon:\n",
    "                rand_values = Q[state]\n",
    "                action = np.argmax(rand_values)-1\n",
    "            else:\n",
    "                action = np.random.randint(n_actions)-1\n",
    "            \n",
    "            picc, nxt_state = returnNext((state//10,state%10), action)\n",
    "            \n",
    "            new_state =nxt_state[0]*10+nxt_state[1]\n",
    "            \n",
    "            old_networth = networth\n",
    "            \n",
    "            reward =0\n",
    "            price+=picc\n",
    "            if action==1:\n",
    "                inventory.append(price)\n",
    "                balance-=10*price\n",
    "                bondbal+=10\n",
    "            elif action ==-1 and len(inventory)>0:\n",
    "                balance+=10*price\n",
    "                bondbal-=10\n",
    "            \n",
    "            networth = balance + bondbal*price\n",
    "            delta = networth - old_networth\n",
    "            reward =delta\n",
    "#             reward = max(delta,0)\n",
    "            \n",
    "            Q[state][action+1] = ((1-alpha)*Q[state][action+1]) + alpha*(reward + gamma*np.max(Q[new_state]))\n",
    "            \n",
    "            cum_reward+=reward\n",
    "            state = new_state\n",
    "        \n",
    "        \n",
    "        if epsilon > epsilon_final:\n",
    "                epsilon*=(1-epsilon_decay)\n",
    "                epsilons.append(epsilon)\n",
    "        cum_rewards.append(cum_reward)\n",
    "        \n",
    "ifTest  = False\n",
    "Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 : mean espiode reward:  49.11904672413675\n",
      "200 : mean espiode reward:  349.2485834054503\n",
      "300 : mean espiode reward:  534.385292749731\n",
      "400 : mean espiode reward:  352.3179218328124\n",
      "500 : mean espiode reward:  552.8845137752828\n",
      "600 : mean espiode reward:  526.0223179269005\n",
      "700 : mean espiode reward:  307.52745904164055\n",
      "800 : mean espiode reward:  317.5044657822038\n",
      "900 : mean espiode reward:  417.02092773948755\n",
      "1000 : mean espiode reward:  441.8270045816905\n",
      "1100 : mean espiode reward:  400.73893969254533\n",
      "1200 : mean espiode reward:  443.4679949025416\n",
      "1300 : mean espiode reward:  322.442898222761\n",
      "1400 : mean espiode reward:  382.182276318695\n",
      "1500 : mean espiode reward:  582.738776176726\n",
      "1600 : mean espiode reward:  436.2370922646428\n",
      "1700 : mean espiode reward:  417.1550423424757\n",
      "1800 : mean espiode reward:  242.71251032210247\n",
      "1900 : mean espiode reward:  300.01318830082977\n",
      "2000 : mean espiode reward:  576.0827472672352\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print((i+1)*100,\": mean espiode reward: \",\\\n",
    "           np.mean(cum_rewards[100*i:100*(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# This you need to write after training your MDP, this should just take in the state and return the Action you would perform\n",
    "# Please do not make use of the returnNext function inside here, that would defeat the purpose of training the model, as it would be known to you!\n",
    "\n",
    "def Run(state):\n",
    "    global epsilon\n",
    "    s = state[0]*10+ state[1]%10\n",
    "    action =getState(s, epsilon)\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main function, you don't need to tamper with it!\n",
    "def mainRun(iter = 1000):\n",
    "    initstate = (random.randint(0,9),random.randint(0,9))\n",
    "    i = 0\n",
    "    initprice = 100.00\n",
    "    price = initprice\n",
    "    balance = 0\n",
    "    bondbal = 0\n",
    "    networth = 0\n",
    "    st = initstate\n",
    "    while i < iter:\n",
    "        act = Run(st)\n",
    "        pricCh, ns = returnNext(st,act)\n",
    "        price += pricCh\n",
    "        if act==1:\n",
    "            balance -= 10*price\n",
    "            bondbal += 10\n",
    "        elif act==-1:\n",
    "            balance += 10*price\n",
    "            bondbal -= 10\n",
    "        \n",
    "        networth = balance + bondbal*price\n",
    "        st = ns\n",
    "        if i%(iter//10) ==0:\n",
    "            print('Your Networth has went from 0 to ',networth)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Networth has went from 0 to  0.0\n",
      "Your Networth has went from 0 to  730744.8454548081\n",
      "Your Networth has went from 0 to  2353840.1413768167\n",
      "Your Networth has went from 0 to  6081417.989759993\n",
      "Your Networth has went from 0 to  12805333.09741686\n",
      "Your Networth has went from 0 to  20995564.663714126\n",
      "Your Networth has went from 0 to  30838088.83696808\n",
      "Your Networth has went from 0 to  42401207.37087433\n",
      "Your Networth has went from 0 to  55689371.26835581\n",
      "Your Networth has went from 0 to  70429604.51895714\n"
     ]
    }
   ],
   "source": [
    "ifTest =True\n",
    "mainRun(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the output above, for **iter=100000**, the networth keeps on increasing in long term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Q3.npy',Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.load('Q.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
